{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "994b8cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcf7ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from runner import Runner\n",
    "import interest_evolution\n",
    "from full_slate_q_agent import FullSlateQAgent\n",
    "from ppo  import PPOAgentWrapper\n",
    "from bandit import EpsilonGreedyBandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc47df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dqn_agent(env, eval_mode=False, summary_writer=None):\n",
    "    return FullSlateQAgent(\n",
    "        observation_space=env.observation_space,\n",
    "        action_space=env.action_space,\n",
    "        eval_mode=eval_mode,\n",
    "        summary_writer=summary_writer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24a90a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "env_config = {\n",
    "    'num_candidates': 10,\n",
    "    'slate_size': 2,\n",
    "    'resample_documents': True,\n",
    "    'seed': seed,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb720d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN][Step 71] AvgLen: 71.00 | AvgRew: 176.00 | StdRew: 0.00 | Time/Step: 0.0007\n",
      "[TRAIN][Step 145] AvgLen: 74.00 | AvgRew: 132.84 | StdRew: 0.00 | Time/Step: 0.0010\n",
      "[TRAIN][Step 221] AvgLen: 76.00 | AvgRew: 176.00 | StdRew: 0.00 | Time/Step: 0.0011\n",
      "[TRAIN][Step 290] AvgLen: 69.00 | AvgRew: 171.84 | StdRew: 0.00 | Time/Step: 0.0014\n",
      "[TRAIN][Step 380] AvgLen: 90.00 | AvgRew: 142.29 | StdRew: 0.00 | Time/Step: 0.0013\n",
      "[EVAL] ckpt_0.pkl | Episode 1 | Reward: 159.57 | Length: 79\n",
      "[EVAL] ckpt_0.pkl | Episode 2 | Reward: 136.19 | Length: 70\n",
      "[EVAL][Step 71] AvgLen: 74.50 | AvgRew: 147.88 | StdRew: 11.69 | Time/Step: 0.0000\n",
      "[EVAL] ckpt_1.pkl | Episode 1 | Reward: 194.38 | Length: 82\n",
      "[EVAL] ckpt_1.pkl | Episode 2 | Reward: 149.23 | Length: 80\n",
      "[EVAL][Step 145] AvgLen: 81.00 | AvgRew: 171.81 | StdRew: 22.58 | Time/Step: 0.0000\n",
      "[EVAL] ckpt_2.pkl | Episode 1 | Reward: 161.71 | Length: 103\n",
      "[EVAL] ckpt_2.pkl | Episode 2 | Reward: 168.44 | Length: 88\n",
      "[EVAL][Step 221] AvgLen: 95.50 | AvgRew: 165.07 | StdRew: 3.37 | Time/Step: 0.0000\n",
      "[EVAL] ckpt_3.pkl | Episode 1 | Reward: 156.45 | Length: 67\n",
      "[EVAL] ckpt_3.pkl | Episode 2 | Reward: 162.33 | Length: 86\n",
      "[EVAL][Step 290] AvgLen: 76.50 | AvgRew: 159.39 | StdRew: 2.94 | Time/Step: 0.0000\n",
      "[EVAL] ckpt_4.pkl | Episode 1 | Reward: 156.00 | Length: 75\n",
      "[EVAL] ckpt_4.pkl | Episode 2 | Reward: 147.87 | Length: 70\n",
      "[EVAL][Step 380] AvgLen: 72.50 | AvgRew: 151.93 | StdRew: 4.07 | Time/Step: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "tmp_base_dir = './logs/dqn'\n",
    "\n",
    "# Automatically delete train and eval directories if they exist\n",
    "train_log_dir = os.path.join(tmp_base_dir, 'train')\n",
    "eval_log_dir = os.path.join(tmp_base_dir, 'eval')\n",
    "\n",
    "for log_dir in [train_log_dir, eval_log_dir]:\n",
    "    if os.path.exists(log_dir):\n",
    "        shutil.rmtree(log_dir)\n",
    "\n",
    "# Recreate base log directory\n",
    "Path(tmp_base_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Initialize training runner ---\n",
    "runner_dqn = Runner(\n",
    "    base_dir=tmp_base_dir,\n",
    "    create_agent_fn=create_dqn_agent,\n",
    "    env=interest_evolution.create_environment(env_config),\n",
    ")\n",
    "\n",
    "# --- Run training + evaluation ---\n",
    "runner_dqn.run_training(max_training_steps=50, num_iterations=5)\n",
    "runner_dqn.run_evaluation(max_eval_episodes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49a4683f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN][Step 82] AvgLen: 82.00 | AvgRew: 174.66 | StdRew: 0.00 | Time/Step: 0.0001\n",
      "[TRAIN][Step 167] AvgLen: 85.00 | AvgRew: 144.00 | StdRew: 0.00 | Time/Step: 0.0001\n",
      "[TRAIN][Step 266] AvgLen: 99.00 | AvgRew: 172.03 | StdRew: 0.00 | Time/Step: 0.0001\n",
      "[TRAIN][Step 347] AvgLen: 81.00 | AvgRew: 164.00 | StdRew: 0.00 | Time/Step: 0.0001\n",
      "[TRAIN][Step 423] AvgLen: 76.00 | AvgRew: 140.93 | StdRew: 0.00 | Time/Step: 0.0001\n",
      "[EVAL] CurrentAgent | Episode 1 | Reward: 182.09 | Length: 94\n",
      "[EVAL] CurrentAgent | Episode 2 | Reward: 152.89 | Length: 84\n",
      "[EVAL][Step 0] AvgLen: 89.00 | AvgRew: 167.49 | StdRew: 14.60 | Time/Step: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from random_agent import RandomAgent\n",
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def create_random_agent(env, **kwargs):\n",
    "    return RandomAgent(action_space=env.action_space)\n",
    "\n",
    "tmp_base_dir = './logs/random'\n",
    "\n",
    "# Automatically delete train and eval directories if they exist\n",
    "train_log_dir = os.path.join(tmp_base_dir, 'train')\n",
    "eval_log_dir = os.path.join(tmp_base_dir, 'eval')\n",
    "\n",
    "for log_dir in [train_log_dir, eval_log_dir]:\n",
    "    if os.path.exists(log_dir):\n",
    "        shutil.rmtree(log_dir)\n",
    "\n",
    "# Recreate base log directory\n",
    "Path(tmp_base_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Initialize runner for RandomAgent ---\n",
    "runner_random = Runner(\n",
    "    base_dir=tmp_base_dir,\n",
    "    create_agent_fn=create_random_agent,\n",
    "    env=interest_evolution.create_environment(env_config),\n",
    ")\n",
    "\n",
    "# --- Run training + evaluation ---\n",
    "runner_random.run_training(max_training_steps=50, num_iterations=5)\n",
    "runner_random.run_evaluation(max_eval_episodes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcf3b939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN][Step 97] AvgLen: 97.00 | AvgRew: 151.31 | StdRew: 0.00 | Time/Step: 0.0002\n",
      "[TRAIN][Step 205] AvgLen: 108.00 | AvgRew: 128.00 | StdRew: 0.00 | Time/Step: 0.0002\n",
      "[TRAIN][Step 331] AvgLen: 126.00 | AvgRew: 167.38 | StdRew: 0.00 | Time/Step: 0.0002\n",
      "[TRAIN][Step 434] AvgLen: 103.00 | AvgRew: 139.97 | StdRew: 0.00 | Time/Step: 0.0002\n",
      "[TRAIN][Step 580] AvgLen: 146.00 | AvgRew: 124.00 | StdRew: 0.00 | Time/Step: 0.0002\n",
      "[EVAL] CurrentAgent | Episode 1 | Reward: 143.54 | Length: 96\n",
      "[EVAL] CurrentAgent | Episode 2 | Reward: 132.16 | Length: 98\n",
      "[EVAL][Step 0] AvgLen: 97.00 | AvgRew: 137.85 | StdRew: 5.69 | Time/Step: 0.0000\n"
     ]
    }
   ],
   "source": [
    "def create_ppo_agent(env, **kwargs):\n",
    "    obs_size = env.observation_space['user'].shape[0]\n",
    "    act_size = env.action_space.nvec[0]\n",
    "    return PPOAgentWrapper(input_dim=obs_size, output_dim=act_size, action_space=env.action_space)\n",
    "\n",
    "tmp_base_dir = './logs/ppo'\n",
    "\n",
    "# Delete and recreate directories\n",
    "train_log_dir = os.path.join(tmp_base_dir, 'train')\n",
    "eval_log_dir = os.path.join(tmp_base_dir, 'eval')\n",
    "for log_dir in [train_log_dir, eval_log_dir]:\n",
    "    if os.path.exists(log_dir):\n",
    "        shutil.rmtree(log_dir)\n",
    "Path(tmp_base_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Run PPO agent\n",
    "runner_ppo = Runner(\n",
    "    base_dir=tmp_base_dir,\n",
    "    create_agent_fn=create_ppo_agent,\n",
    "    env=interest_evolution.create_environment(env_config),\n",
    ")\n",
    "\n",
    "runner_ppo.run_training(max_training_steps=50, num_iterations=5)\n",
    "runner_ppo.run_evaluation(max_eval_episodes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22edeecf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EpsilonGreedyBandit' object has no attribute 'begin_episode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 22\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Run Bandit agent\u001b[39;00m\n\u001b[1;32m     16\u001b[0m runner_bandit \u001b[38;5;241m=\u001b[39m Runner(\n\u001b[1;32m     17\u001b[0m     base_dir\u001b[38;5;241m=\u001b[39mtmp_base_dir,\n\u001b[1;32m     18\u001b[0m     create_agent_fn\u001b[38;5;241m=\u001b[39mcreate_bandit_agent,\n\u001b[1;32m     19\u001b[0m     env\u001b[38;5;241m=\u001b[39minterest_evolution\u001b[38;5;241m.\u001b[39mcreate_environment(env_config),\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m \u001b[43mrunner_bandit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_training_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m runner_bandit\u001b[38;5;241m.\u001b[39mrun_evaluation(max_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/recsim_2025/runner.py:142\u001b[0m, in \u001b[0;36mRunner.run_training\u001b[0;34m(self, max_training_steps, num_iterations, checkpoint_frequency)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m num_steps \u001b[38;5;241m<\u001b[39m max_training_steps:\n\u001b[1;32m    141\u001b[0m     episode_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 142\u001b[0m     ep_len, ep_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     episode_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m episode_start\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_length\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(ep_len)\n",
      "File \u001b[0;32m~/Desktop/recsim_2025/runner.py:72\u001b[0m, in \u001b[0;36mRunner._run_one_episode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     71\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 72\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_episode\u001b[49m(observation)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m step_number \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_steps_per_episode:\n\u001b[1;32m     75\u001b[0m     last_observation \u001b[38;5;241m=\u001b[39m observation\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EpsilonGreedyBandit' object has no attribute 'begin_episode'"
     ]
    }
   ],
   "source": [
    "def create_bandit_agent(env, **kwargs):\n",
    "    act_size = env.action_space.nvec[0]\n",
    "    return EpsilonGreedyBandit(n_arms=act_size, epsilon=0.1)\n",
    "\n",
    "tmp_base_dir = './logs/bandit'\n",
    "\n",
    "# Delete and recreate directories\n",
    "train_log_dir = os.path.join(tmp_base_dir, 'train')\n",
    "eval_log_dir = os.path.join(tmp_base_dir, 'eval')\n",
    "for log_dir in [train_log_dir, eval_log_dir]:\n",
    "    if os.path.exists(log_dir):\n",
    "        shutil.rmtree(log_dir)\n",
    "Path(tmp_base_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Run Bandit agent\n",
    "runner_bandit = Runner(\n",
    "    base_dir=tmp_base_dir,\n",
    "    create_agent_fn=create_bandit_agent,\n",
    "    env=interest_evolution.create_environment(env_config),\n",
    ")\n",
    "\n",
    "runner_bandit.run_training(max_training_steps=50, num_iterations=5)\n",
    "runner_bandit.run_evaluation(max_eval_episodes=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
